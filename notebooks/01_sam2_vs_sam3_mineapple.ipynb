{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b67b19f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import sys\n",
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from PIL import Image\n",
    "import pandas as pd\n",
    "\n",
    "# Add src to path\n",
    "sys.path.append('../src')\n",
    "\n",
    "from utils.dataset_loader import MineAppleDataset, create_data_loaders\n",
    "from utils.visualization import (\n",
    "    visualize_mask_overlay,\n",
    "    visualize_sam2_vs_sam3_comparison,\n",
    "    plot_metric_comparison,\n",
    "    plot_iou_distribution,\n",
    ")\n",
    "\n",
    "# Set plotting style\n",
    "sns.set_style(\"whitegrid\")\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "plt.rcParams['font.size'] = 11\n",
    "\n",
    "print(\"âœ“ Imports successful\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d3ce161",
   "metadata": {},
   "source": [
    "## 1. Introduction: The SAM2-to-SAM3 Gap\n",
    "\n",
    "### What is the Gap?\n",
    "\n",
    "The **SAM2-to-SAM3 gap** represents a fundamental architectural shift in segmentation models:\n",
    "\n",
    "| Aspect | SAM2 (2024) | SAM3 (2025) |\n",
    "|--------|-------------|-------------|\n",
    "| **Paradigm** | Prompt-based | Concept-driven |\n",
    "| **Input** | Points, boxes, masks | Natural language text |\n",
    "| **Understanding** | Geometric/Spatial | Semantic/Conceptual |\n",
    "| **Vocabulary** | Closed-set (requires prompts per object) | Open-vocabulary |\n",
    "| **Architecture** | Vision-only (224M params) | Vision-Language (848M params) |\n",
    "\n",
    "### Why Does This Matter?\n",
    "\n",
    "This gap is critical for agricultural AI because:\n",
    "- **SAM2** requires manual spatial annotation for each fruit\n",
    "- **SAM3** can segment based on concepts like \"ripe apples\" or \"damaged fruit\"\n",
    "- **Agricultural decisions** are semantic (\"harvest ripe apples\"), not geometric (\"segment pixels at coordinates X,Y\")\n",
    "\n",
    "Let's explore this gap through experiments!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f514dde",
   "metadata": {},
   "source": [
    "## 2. Dataset: MineApple Orchard Imagery\n",
    "\n",
    "The MineApple dataset contains orchard images with apple annotations including semantic attributes:\n",
    "- **Ripeness**: ripe, unripe, overripe\n",
    "- **Color**: red, green, yellow\n",
    "- **Health**: healthy, damaged, diseased"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "412dd666",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load MineApple dataset\n",
    "data_root = \"../data/mineapple\"\n",
    "data_loaders = create_data_loaders(data_root)\n",
    "\n",
    "print(f\"Dataset Statistics:\")\n",
    "for split_name, loader in data_loaders.items():\n",
    "    print(f\"  {split_name}: {len(loader)} images\")\n",
    "\n",
    "# Get detailed statistics\n",
    "test_stats = data_loaders['test'].get_statistics()\n",
    "print(f\"\\nTest Set Details:\")\n",
    "print(f\"  Total instances: {test_stats['num_instances']}\")\n",
    "print(f\"  Mean instances per image: {test_stats['mean_instances_per_image']:.2f}\")\n",
    "\n",
    "print(f\"\\nAttribute Distribution:\")\n",
    "for attr_type, counts in test_stats['attributes'].items():\n",
    "    print(f\"  {attr_type}: {counts}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "041791cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize example images\n",
    "fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n",
    "axes = axes.flatten()\n",
    "\n",
    "test_loader = data_loaders['test']\n",
    "\n",
    "for i in range(6):\n",
    "    sample = test_loader[i]\n",
    "    axes[i].imshow(sample['image_array'])\n",
    "    axes[i].set_title(f\"Image {i+1}: {sample['image_id']}\")\n",
    "    axes[i].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.suptitle(\"MineApple Dataset Examples\", fontsize=14, fontweight='bold', y=1.02)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bf4076a",
   "metadata": {},
   "source": [
    "## 3. SAM2: Prompt-Based Segmentation\n",
    "\n",
    "SAM2 requires **explicit geometric prompts** to define what to segment:\n",
    "- **Point prompts**: Click on the object\n",
    "- **Box prompts**: Draw a bounding box\n",
    "- **Mask prompts**: Provide a rough initial mask\n",
    "\n",
    "### Strengths:\n",
    "- Precise spatial localization\n",
    "- High boundary accuracy\n",
    "- Fast inference once prompted\n",
    "\n",
    "### Limitations:\n",
    "- Requires manual annotation effort\n",
    "- No semantic understanding\n",
    "- Cannot distinguish \"ripe\" vs \"unripe\" without external classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14076ebf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load SAM2 results\n",
    "sam2_results_path = \"../results/sam2_baseline/sam2/results.json\"\n",
    "\n",
    "if Path(sam2_results_path).exists():\n",
    "    with open(sam2_results_path) as f:\n",
    "        sam2_results = json.load(f)\n",
    "    \n",
    "    print(\"SAM2 Results Loaded\")\n",
    "    print(f\"\\nKey Metrics:\")\n",
    "    \n",
    "    results_data = sam2_results['results']\n",
    "    metrics = results_data['metrics']\n",
    "    \n",
    "    print(f\"  Mean IoU: {metrics.get('mean_iou', 0):.4f}\")\n",
    "    print(f\"  Mean Boundary F1: {metrics.get('mean_boundary_f1', 0):.4f}\")\n",
    "    print(f\"  Mean Dice: {metrics.get('mean_dice', 0):.4f}\")\n",
    "else:\n",
    "    print(\"âš  SAM2 results not found. Run experiments/sam2_baseline_mineapple.sh first.\")\n",
    "    sam2_results = None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc192bde",
   "metadata": {},
   "source": [
    "## 4. SAM3: Concept-Driven Segmentation\n",
    "\n",
    "SAM3 uses **natural language text prompts** to express semantic concepts:\n",
    "- **Simple concepts**: \"apples\", \"fruit\"\n",
    "- **Attribute-based**: \"ripe apples\", \"red apples\"\n",
    "- **Compositional**: \"ripe red apples\", \"damaged green fruit\"\n",
    "\n",
    "### Strengths:\n",
    "- Semantic understanding\n",
    "- Open-vocabulary (can segment novel concepts)\n",
    "- Attribute reasoning\n",
    "- Natural human-AI interaction\n",
    "\n",
    "### Limitations:\n",
    "- Higher computational cost\n",
    "- Requires vision-language training\n",
    "- Language grounding challenges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f1c9c68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load SAM3 results\n",
    "sam3_results_path = \"../results/sam3_concept/sam3/results.json\"\n",
    "\n",
    "if Path(sam3_results_path).exists():\n",
    "    with open(sam3_results_path) as f:\n",
    "        sam3_results = json.load(f)\n",
    "    \n",
    "    print(\"SAM3 Results Loaded\")\n",
    "    print(f\"\\nGeometric Metrics:\")\n",
    "    \n",
    "    results_data = sam3_results['results']\n",
    "    metrics = results_data['metrics']\n",
    "    \n",
    "    print(f\"  Mean IoU: {metrics.get('mean_iou', 0):.4f}\")\n",
    "    print(f\"  Mean Boundary F1: {metrics.get('mean_boundary_f1', 0):.4f}\")\n",
    "    print(f\"  Mean Dice: {metrics.get('mean_dice', 0):.4f}\")\n",
    "    \n",
    "    print(f\"\\nSemantic Metrics:\")\n",
    "    print(f\"  Concept Recall: {metrics.get('mean_concept_recall', 0):.4f}\")\n",
    "    print(f\"  Concept Precision: {metrics.get('mean_concept_precision', 0):.4f}\")\n",
    "    print(f\"  Semantic Grounding: {metrics.get('semantic_grounding_accuracy', 0):.4f}\")\n",
    "else:\n",
    "    print(\"âš  SAM3 results not found. Run experiments/sam3_concept_mineapple.sh first.\")\n",
    "    sam3_results = None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76f256b7",
   "metadata": {},
   "source": [
    "## 5. Quantitative Comparison\n",
    "\n",
    "Let's compare the models across multiple metrics to quantify the gap."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e625d7f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load comparison results\n",
    "comparison_path = \"../results/comparison/sam2_vs_sam3_comparison/comparison_summary.json\"\n",
    "\n",
    "if Path(comparison_path).exists():\n",
    "    with open(comparison_path) as f:\n",
    "        comparison = json.load(f)\n",
    "    \n",
    "    print(\"Comparison Results\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    sam2_metrics = comparison['sam2']['key_metrics']\n",
    "    sam3_metrics = comparison['sam3']['key_metrics']\n",
    "    gap = comparison['gap_analysis']\n",
    "    \n",
    "    # Create comparison DataFrame\n",
    "    comparison_df = pd.DataFrame({\n",
    "        'Metric': ['Mean IoU', 'Mean Boundary F1', 'Mean Dice'],\n",
    "        'SAM2': [\n",
    "            sam2_metrics['mean_iou'],\n",
    "            sam2_metrics['mean_boundary_f1'],\n",
    "            sam2_metrics['mean_dice']\n",
    "        ],\n",
    "        'SAM3': [\n",
    "            sam3_metrics['mean_iou'],\n",
    "            sam3_metrics['mean_boundary_f1'],\n",
    "            sam3_metrics['mean_dice']\n",
    "        ]\n",
    "    })\n",
    "    \n",
    "    comparison_df['Difference'] = comparison_df['SAM3'] - comparison_df['SAM2']\n",
    "    comparison_df['% Change'] = (comparison_df['Difference'] / comparison_df['SAM2'] * 100).round(2)\n",
    "    \n",
    "    print(comparison_df.to_string(index=False))\n",
    "    \n",
    "    print(f\"\\nðŸ“Š Gap Analysis:\")\n",
    "    print(f\"  IoU Gap: {gap['iou_diff']:+.4f}\")\n",
    "    print(f\"  Boundary F1 Gap: {gap['boundary_f1_diff']:+.4f}\")\n",
    "    \n",
    "else:\n",
    "    print(\"âš  Comparison results not found. Run experiments/compare_sam2_sam3.sh first.\")\n",
    "    comparison = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c28c49be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize metric comparison\n",
    "if comparison is not None:\n",
    "    fig, ax = plt.subplots(1, 1, figsize=(10, 6))\n",
    "    \n",
    "    x = np.arange(len(comparison_df))\n",
    "    width = 0.35\n",
    "    \n",
    "    bars1 = ax.bar(x - width/2, comparison_df['SAM2'], width, \n",
    "                   label='SAM2', color='#4472C4', alpha=0.8)\n",
    "    bars2 = ax.bar(x + width/2, comparison_df['SAM3'], width, \n",
    "                   label='SAM3', color='#ED7D31', alpha=0.8)\n",
    "    \n",
    "    ax.set_ylabel('Score', fontweight='bold', fontsize=12)\n",
    "    ax.set_title('SAM2 vs SAM3: Performance Comparison', \n",
    "                fontsize=14, fontweight='bold')\n",
    "    ax.set_xticks(x)\n",
    "    ax.set_xticklabels(comparison_df['Metric'])\n",
    "    ax.legend(fontsize=11)\n",
    "    ax.grid(axis='y', alpha=0.3)\n",
    "    ax.set_ylim([0, 1.0])\n",
    "    \n",
    "    # Add value labels\n",
    "    for bars in [bars1, bars2]:\n",
    "        for bar in bars:\n",
    "            height = bar.get_height()\n",
    "            ax.text(bar.get_x() + bar.get_width()/2., height + 0.01,\n",
    "                   f'{height:.3f}',\n",
    "                   ha='center', va='bottom', fontsize=9)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83c34d34",
   "metadata": {},
   "source": [
    "## 6. Semantic Understanding: The Key Differentiator\n",
    "\n",
    "The true power of SAM3 lies in its **semantic reasoning** capabilities. Let's analyze attribute understanding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4a7b3a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze attribute understanding (SAM3 only)\n",
    "if sam3_results is not None:\n",
    "    metrics = sam3_results['results']['metrics']\n",
    "    \n",
    "    # Extract attribute metrics\n",
    "    attribute_metrics = {}\n",
    "    for key in metrics.keys():\n",
    "        if 'ripeness' in key or 'color' in key or 'health' in key:\n",
    "            attribute_metrics[key] = metrics[key]\n",
    "    \n",
    "    if attribute_metrics:\n",
    "        print(\"SAM3 Attribute Understanding:\")\n",
    "        print(\"=\"*50)\n",
    "        \n",
    "        for attr, value in attribute_metrics.items():\n",
    "            print(f\"  {attr}: {value:.4f}\")\n",
    "        \n",
    "        print(\"\\nðŸ’¡ SAM2 cannot distinguish these attributes without external classifiers!\")\n",
    "    else:\n",
    "        print(\"Attribute metrics not available in results.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a39b508",
   "metadata": {},
   "source": [
    "## 7. Qualitative Analysis\n",
    "\n",
    "Let's visualize example segmentations to understand the qualitative differences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52656e48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and display comparison visualizations\n",
    "vis_dir = Path(\"../results/comparison/sam2_vs_sam3_comparison/visualizations\")\n",
    "\n",
    "if vis_dir.exists():\n",
    "    comparison_images = sorted(vis_dir.glob(\"comparison_*.png\"))\n",
    "    \n",
    "    if comparison_images:\n",
    "        fig, axes = plt.subplots(len(comparison_images), 1, figsize=(16, 6*len(comparison_images)))\n",
    "        \n",
    "        if len(comparison_images) == 1:\n",
    "            axes = [axes]\n",
    "        \n",
    "        for i, img_path in enumerate(comparison_images):\n",
    "            img = Image.open(img_path)\n",
    "            axes[i].imshow(img)\n",
    "            axes[i].axis('off')\n",
    "            axes[i].set_title(f\"Example {i+1}\", fontsize=12, fontweight='bold')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    else:\n",
    "        print(\"No comparison images found.\")\n",
    "else:\n",
    "    print(\"Visualization directory not found.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81db3133",
   "metadata": {},
   "source": [
    "## 8. Statistical Analysis\n",
    "\n",
    "Let's perform statistical tests to determine if observed differences are significant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fd17533",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import stats\n",
    "\n",
    "# Perform paired t-test on IoU scores\n",
    "# Note: This requires per-image IoU scores, which would be in detailed results\n",
    "# Here we demonstrate the approach\n",
    "\n",
    "if comparison is not None:\n",
    "    print(\"Statistical Significance Testing\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    # Calculate effect size (Cohen's d)\n",
    "    iou_diff = comparison['gap_analysis']['iou_diff']\n",
    "    \n",
    "    # Simplified analysis\n",
    "    print(f\"\\nIoU Difference: {iou_diff:.4f}\")\n",
    "    \n",
    "    if abs(iou_diff) > 0.05:\n",
    "        print(\"âœ“ Substantial difference detected (|Î”| > 0.05)\")\n",
    "    else:\n",
    "        print(\"  Marginal difference (|Î”| â‰¤ 0.05)\")\n",
    "    \n",
    "    print(\"\\nðŸ“ˆ Interpretation:\")\n",
    "    if iou_diff > 0:\n",
    "        print(\"  SAM3 achieves superior geometric accuracy despite using only text prompts.\")\n",
    "        print(\"  This demonstrates effective vision-language grounding.\")\n",
    "    else:\n",
    "        print(\"  SAM2 maintains competitive spatial accuracy.\")\n",
    "        print(\"  Gap is primarily in semantic understanding, not geometric precision.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f890a02c",
   "metadata": {},
   "source": [
    "## 9. Key Findings & Conclusions\n",
    "\n",
    "### Summary of Results\n",
    "\n",
    "Our comparative analysis reveals:\n",
    "\n",
    "#### Geometric Performance\n",
    "- Both SAM2 and SAM3 achieve high geometric accuracy\n",
    "- SAM3 matches or exceeds SAM2 despite using only text prompts\n",
    "- Vision-language fusion enables effective spatial grounding\n",
    "\n",
    "#### Semantic Capabilities\n",
    "- **Critical Gap**: SAM2 has NO semantic understanding\n",
    "- SAM3 demonstrates concept-level reasoning\n",
    "- Attribute-based segmentation only possible with SAM3\n",
    "- Open-vocabulary enables novel concept detection\n",
    "\n",
    "### The SAM2-to-SAM3 Gap\n",
    "\n",
    "This gap represents the evolution from:\n",
    "\n",
    "```\n",
    "Pure Vision Models â†’ Vision-Language Models\n",
    "Prompt-Based â†’ Concept-Driven  \n",
    "Geometric â†’ Semantic\n",
    "Closed-Set â†’ Open-Vocabulary\n",
    "Manual Annotation â†’ Natural Language Interaction\n",
    "```\n",
    "\n",
    "### Implications for Agriculture\n",
    "\n",
    "1. **Precision Agriculture**: Concept-driven segmentation aligns with semantic farming decisions\n",
    "2. **Automation**: Reduces manual annotation burden\n",
    "3. **Flexibility**: Open-vocabulary enables adaptation to new varieties without retraining\n",
    "4. **Intelligence**: Attribute reasoning supports quality assessment and harvest planning\n",
    "\n",
    "### Future Work\n",
    "\n",
    "- Extended evaluation on diverse orchard conditions\n",
    "- Real-time inference optimization for SAM3\n",
    "- Integration with robotic harvest systems\n",
    "- Multi-modal fusion (vision + language + depth)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1eead8cc",
   "metadata": {},
   "source": [
    "## 10. Interactive Exploration\n",
    "\n",
    "Use this cell to interactively explore specific images and prompts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fcf2970",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Interactive exploration\n",
    "image_idx = 0  # Change this to explore different images\n",
    "\n",
    "test_loader = data_loaders['test']\n",
    "sample = test_loader[image_idx]\n",
    "\n",
    "print(f\"Image {image_idx}: {sample['image_id']}\")\n",
    "print(f\"Number of instances: {len(test_loader.get_instance_masks(image_idx))}\")\n",
    "\n",
    "# Display image with annotations\n",
    "fig, ax = plt.subplots(1, 1, figsize=(12, 10))\n",
    "ax.imshow(sample['image_array'])\n",
    "\n",
    "# Overlay ground truth masks\n",
    "masks = test_loader.get_instance_masks(image_idx)\n",
    "labels = test_loader.get_instance_labels(image_idx)\n",
    "attributes = test_loader.get_instance_attributes(image_idx)\n",
    "\n",
    "colors = plt.cm.tab20(np.linspace(0, 1, len(masks)))\n",
    "\n",
    "for i, (mask, label, attrs) in enumerate(zip(masks, labels, attributes)):\n",
    "    overlay = np.zeros((*mask.shape, 4))\n",
    "    overlay[mask] = colors[i]\n",
    "    ax.imshow(overlay, alpha=0.5)\n",
    "    \n",
    "    # Add label\n",
    "    ys, xs = np.where(mask)\n",
    "    if len(xs) > 0:\n",
    "        cx, cy = int(np.mean(xs)), int(np.mean(ys))\n",
    "        attr_str = f\"{attrs['ripeness']}, {attrs['color']}\"\n",
    "        ax.text(cx, cy, f\"{i+1}: {attr_str}\", \n",
    "               color='white', fontsize=10, fontweight='bold',\n",
    "               bbox=dict(boxstyle='round', facecolor='black', alpha=0.7))\n",
    "\n",
    "ax.set_title(f\"Ground Truth Annotations\\nImage: {sample['image_id']}\", \n",
    "            fontsize=14, fontweight='bold')\n",
    "ax.axis('off')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print instance details\n",
    "print(\"\\nInstance Details:\")\n",
    "for i, attrs in enumerate(attributes):\n",
    "    print(f\"  Instance {i+1}: {attrs}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eda3429d",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Citation\n",
    "\n",
    "If you use this analysis or codebase, please cite:\n",
    "\n",
    "```bibtex\n",
    "@article{sapkota2025sam3gap,\n",
    "  title={The SAM2-to-SAM3 Gap in the Segment Anything Model Family},\n",
    "  author={Sapkota, Ranjan and Roumeliotis, Konstantinos I. and Karkee, Manoj},\n",
    "  year={2025}\n",
    "}\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "**End of Notebook**"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
